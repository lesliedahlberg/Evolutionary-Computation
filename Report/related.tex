\section{Related Work}

Ideas around evolutionary computation began emerging in 1950s. Several researchers, independently from each-other, created algorithms which were inspired by natural Darwinian principles, these include Holland's Genetic Algorithms, Schwefel's and Rechenberg's Evolution Strategies and Fogel's Evolutionary Programming. These pioneering algorithms shared the concepts of populations, individuals, offspring and fitness and ,compared to natural systems, they were quite simplistic, lacking gender, maturation processees, migration, etc \cite{dejong2009EC}.

Research has shown that no single algorithm can perform better than all other algorithms on average. This has been referred to as the `no-free-lunch' and current solutions instead aim at finding better solutions to specific problems by exploiting inherent biases in the problem. This has led to the desire to classify different algorithms in order to decide which algorithms should be used in which situations, a problem which is not trivial \cite{dejong2009EC}.

Recent research has focused, among others, on parallelism, multi-population models, multi-objective optimization, dynamic environments and evolving executable code. Parallelism can easily be exploited in EC because of it's inherently parallel nature, e.g each individual in a population can be evaluated, mutated and crossed-over independently. Multi-core CPUs, massively parallel GPUs, clusters and networks can be used to achieve this. Multi-population models mimic the way species depend on each other in nature. Examples of this include host-parasite and predator-prey relationships where the the individual's fitness is connected to the fitness of another individual. Multi-objective optimization aims to solve problems where conflicting interests exist, a good example would be optimizing for power and fuel-consumption simultaneously. In such problems the optimization algorithm has to keep two or more interests in mind simultaneously and find intersections points which offer the best trade-offs between them. Dynamic environments include things like the stock markets and traffic systems. Traditional EAs perform badly in these situations but they can perform well when slightly modified to fit the task. Evolving executable code, as in Genetic Programming and Evolutionary Programming, is a hard problem with very interesting potential applications. Most often low-level code such as assembly, lisp or generic rules are evolved \cite{dejong2009EC}.

\subsection{Differential Evolution}

Differential evolution (DE) was conceived in 1995 by Storn and Price \cite{storn1995differential} and soon gained wide acceptance as one of the best algorithms in continuous optimization \cite{price1997differential}. This spawned many new papers descibing variations and hybrids of the algorithm \cite{5601760}, such as self-adaptive DE (SaDE) \cite{qin2009differential}, opposition-based DE (ODE) \cite{rahnamayan2008opposition} and DE with global and local neighborhoods (DEGL) \cite{rahnamayan2008opposition}.

DE is very easy to implement and has been shown to outperform most other algorithms consistently, it has also been shown that it in general performs better than PSO \cite{das2009differential, vesterstrom2004comparative}. DE uses very few patameters and the effects of altering these parameters have been well studied \cite{5601760}. DE comes in a total of 10 different varieties based on which mutation and cross-over operators are used \cite{price2006differential}. Eight of these schemes have been tested and compared, showing that the version called DE/best/1/bin (which utilizes the best current individual in the cross-over process instead of random invidivuals) generally yields the best results \cite{mezura2006comparative}. \cite{gamperle2002parameter} measured the performance of different combinations of parameters, producing general recommendations for DE.

The desire to find optimal parameters have led to the use of self-adjusting DE algorithms. Examples inlude the use of fuzzy systems to control the parameters \cite{liu2005fuzzy} and the SaDE algorithm \cite{qin2009differential}.

\subsection{Particle Swarm Optimization}

Particle swarm optimization (PSO) is the most widely used swarm intelligence (SI) algorithm to date. Many modified versions of PSO have been proposed, among others quantum-behaved PSO (QPSO), bare-bones PSO (BBPSO), chaotic PSO, fuzzy PSO, PSOT-VAC and opposition-based PSO. PSO has also been hybridized with other evolutionary algorithm, for instance: genetic algorithms (GA), artificial immune systems (AIS), tabu search (TS), ant colony optimization (ACO), simulated annealing (SA), differential evolution (DE), bio-geography based optimization (BBO) and harmonic search (HS) \cite{zhang2015comprehensive}.

Wang et al. \cite{dang2012selection} have compared the performance of different PSO-parameters and proposed a set of criteria for improving the performance of PSO. Fuzzy logic controllers (FLC) have been used to continuously optimize the PSO-parameters by Kumar and Chaturvedi \cite{kumar2011tuning}. Zhang et al. found a simple way to use control theory in order to find good parameters \cite{zhang2011simple}. Yang proposed modified velocity PSO (MVPSO) in which particles learn the best parameters from the other particles \cite{yang2011particle}.


\subsection{Estimation of Distribution Algorithms}

\subsection{Evolutionary Algorithms in Machine Learning}




{\color{blue}
\begin{itemize}
  \item Syftet med detta avsnitt \"ar att placera in ditt arbete i ett sammanhang och j\"amf\"ora det med tidigare publicerade arbeten och resultat inom omr\r{a}det. Denna del ska vara grundlig. Du beskriver h\"ar existerande kunskap och hur denna ut\"okas av ditt arbete. Den ska inneh\r{a}lla analyser av tidigare arbeten som exempelvis beskriver hur olika metoder skiljer sig \r{a}t. Du ska visa p\r{a} de viktigaste likheterna och skillnaderna betr\"affande uppgift, angreppss\"att/metodologi samt resultat. Det \"ar viktigt att du p\r{a} ett neutralt s\"att diskuterar f\"or- och nackdelar med ditt eget arbete j\"amf\"ort med andras.
  \item Detta skapar ocks\r{a} en f\"orv\"antan p\r{a} bidraget f\"or ditt arbete, l\"asaren l\"ar sig h\"ar om begr\"ansningar hos tidigare arbeten och varf\"or din uppgift \"ar en utmaning..
  \item Tillsammans kommer detta avsnitt tillsammans med bakgrund att introducera ``state of the art''/``state of practice'' och dess brister, betydelsen av uppgiften samt vad ditt arbete ska j\"amf\"oras med.
\end{itemize}




}
